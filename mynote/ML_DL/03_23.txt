[4] logistic regression ( binary classification )
:독립 변수가 두 개의 그룹으로 나뉨
ex)
#training set
x_data = np.array([[10,0],
                  [8,1],
                  [3,3],
                  [2,3],
                  [5,1],
                  [2,0],
                  [1,0]])
y_data = np.array([[1],[1],[1],[1],[0],[0],[0],]) #두개의 그룹으로 나뉨

#placeholder
X = tf.placeholder(shape=[None,2], dtype=tf.float32) #2열
Y = tf.placeholder(shape=[None,1], dtype=tf.float32) #1열

#weight and bias
W = tf.Variable(tf.random_normal([2,1]), name="weight") 7행2열 @ w = 7행 1열 > 2행 1열
b = tf.Variable(tf.random_normal([1]), name="bias")

#hypothesis
logits = tf.matmul(X, W) + b
H = tf.sigmoid(logits) #sigmoid 사용

#cost function
#cost = tf.reduce_mean(tf.square(H-Y))
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))

#train
train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

#session
sess = tf.Session()
sess.run(tf.global_variables_initializer())

#학습
for step in range(1,3001):
    _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})
    if step%300==0:
        print("{}번째 W: {}".format(step, cost_val))

# Accuracy
predict = tf.cast(H>0.5, dtype=tf.float32)
correct = tf.equal(predict, Y) # True True True True False True True
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
print('정확도 :',sess.run(accuracy, feed_dict={X:x_data, Y:y_data}) )

[5]multinomial classification
:독립 변수가 3개 이상의 그룹으로 나뉨

ex)
#training set
x_data = np.array([[10,7,8,5],
                  [8,8,9,4],
                  [7,8,2,3],
                  [6,3,9,3],
                  [7,5,7,4],
                  [3,5,6,2],
                  [2,4,3,1]]) #7*4
#종속변수는 원핫인코딩
y_data = np.array([[1,0,0],[1,0,0],[0,1,0],[0,1,0],[0,1,0],[0,0,1],[0,0,1],])# 7*3개의 그룹

#placeholder
X = tf.placeholder(shape=[None,4], dtype=tf.float32)
Y = tf.placeholder(shape=[None,3], dtype=tf.float32)

#weight and bias
W = tf.Variable(tf.random_normal([4,3]), name="weight")# 4*3
b = tf.Variable(tf.random_normal([3]), name="bias")

#hypothesis
logits = tf.matmul(X, W) + b
#H = tf.sigmoid(logits) > binary
H = tf.nn.softmax(logits) # multi > softmax 사용

#cost function
#cost = tf.reduce_mean(tf.square(H-Y))
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))

#이하 동일

[6] XOR gate with version1 (deep learning, layer 증폭)
:version 1으로는 분류가 불가능, 다단 레이어 증폭을 통한 딥러닝으로 정확도 보정 가능
ex)
# training data set
x_data = [[0,0], [0,1], [1,0], [1,1]]
y_data = [[0],   [1],   [1],   [0]]
# placeholder
X = tf.placeholder(shape=[None, 2], dtype=tf.float32)
Y = tf.placeholder(shape=[None, 1], dtype=tf.float32)

# layer 추가
# Weight & bias(layer1= 입력2개 출력 4)
W1 = tf.Variable(tf.random_normal([2,10]), name="weight1")
b1 = tf.Variable(tf.random_normal([10]), name="bias1")
layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)
# Weight & bias(layer2 = 입력10개, 출력20개)
W2 = tf.Variable(tf.random_normal([10,20]), name="weight2")
b2 = tf.Variable(tf.random_normal([20]), name="bias2")
layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)
# Weight & bias(layer3 = 입력20개, 출력10개)
W3 = tf.Variable(tf.random_normal([20,10]), name="weight3")
b3 = tf.Variable(tf.random_normal([10]), name="bias3")
layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)
# Weight & bias(output layer = 입력10개, 출력1개)
W4 = tf.Variable(tf.random_normal([10,1]), name="weight4")
b4 = tf.Variable(tf.random_normal([1]), name="bias4")
#은닉층에는 relu 사용

# Hypothesis
logits = tf.matmul(layer3, W4) + b4
H = tf.sigmoid(logits)

# cost
cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))
#이하 동일

[7] XOR gate with version2
ex)
#1. training data set
x_data = [[0,0], [0,1], [1,0], [1,1]]
y_data = [[0],   [1],   [1],   [0]]

#2. 모델 구성
model = Sequential()
model.add(Dense(10,input_dim=2, activation="relu"))
model.add(Dense(20, activation="relu"))# input_dim 자동
model.add(Dense(10, activation="relu"))
model.add(Dense(1, activation="relu"))
model.summary()

#3. 모델 학습과정
model.compile(loss="mse", optimizer="adam", metrics=['binary_accuracy'])

#4. 학습하기
fit_hist = model.fit(x_data, y_data, epochs=100, verbose=1)