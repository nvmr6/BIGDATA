[14] 웹 데이터 수집
1. BeautifulSoup
: screen scraping 프로젝트를 위해 설계된 파이썬 라이브러리
-구문 분석 , 트리 탐색 , 검색 및 수정을 위한 몇 가지 간단한 방법과 파이썬 관
용구를 제공하며 문서를 분석하고 필요한 것을 추출하는 도구
-들어오는 문서를 유니코드로 보내고 문서를 UTF 8 로 자동 변환
-뷰티풀솝은 lxml 및 html5lib 과 같은 파이썬 파서 라이브러리를 사용할 수 있
음
-가장 일반적으로 사용되는 CSS 선택자를 지원
ex)
import requests
from bs4 import BeautifulSoup
response= requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

Selector API
-select 와 select_one () 메서드만 알아도 원하는 요소를 찾기에 충분
-soup.select (“CSS 선택자 ””) : CSS 선택자에 해당하는 모든 요소를 반환
-soup.select_one (“CSS 선택자 ””) : CSS 선택자에 맞는 오직 첫 번째 태그 요소만 반환
ex)
soup.select_one(CSS선택자)

*CSS 선택자
:CSS 를 이용해서 HTML 문서에 시각적 요소를 부여할 때 문서 내의 어느 요소에 부여할지를 결정하기 위해 사용하는 것
>CSS 선택자는 HTML 문서의 태그 이름 , class 속성 , id 속성, 후손/자손 선택자 등을 이용해서 작성할 수 있음

2.requests
:파이썬에서 HTTP 요청을 만들기 위한 사실상의 표준
>import requests

get(): HTTP 요청 (Request) 방식 중 하나
>response= requests.get(url) > 요청의 결과를 저장한 객체
>.content: 바이트 단위로, text: 문자열 인코딩 

ex1)
url = "https://movie.naver.com/movie/sdb/rank/rmovie.nhn"
movie_ranking = requests.get(url)
soup = BeautifulSoup(movie_ranking.content, "html.parser")
title_list = soup.select("td.title div.tit3 > a")
for i in range(len(title_list)):
    print("{:2d}위 {}".format(i+1, title_list[i].attrs['title']))
    link = url+title_list[i].attrs['href']
    s = BeautifulSoup(requests.get(link).content, "html.parser")
    people = s.select("li a.tx_people")
    staffs = s.select("dl.staff > dt")
    print("감독 및 배우")
    for idx in range(len(people)):
        print("{}: {}".format(staffs[idx].text, people[idx].text), end=" | ")
    print("\n")

3. 정규표현식 이용
- \d (숫자와 매치. [0-9] 동일) 
- \D (숫자가 아닌 것)
- \s (whitespace)
- \w (영문자나 숫자)
- \W (영문자나 숫자가 아닌 문자)
- .  (문자 하나)

>>- (1번이상 반복)
    - (0번이상 반복) {2,4} (2~4회 반복) ? ({0,1}을 의미) </pre> ### 정표현식 예
    - 메일 정규표현식 \w+@\w+.\w+[.\w+]*
    - 전화번호 정규표현식 .?\d{2,3}.?\d{3,4}.?\d{4}
    - ip주소 정규표현식 [0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}.[0-9]{1,3}
    - 주민번호 정규표현식 [0-9]{2}[0-1][0-9][0-3][0-9]-[1-4]\d{6}

4. selenium 이용
:브라우저의 동작을 자동화 해주는 프로그램

*셀레니움 파이썬 바인딩
:셀레니움 WebDriver를 사용하여 파이어폭스 (Firefox), 인터넷 익스플로러 Ie ),
크롬 (Chrome) 등 브라우저에 접근하고 브라우저의 동작을 제어 할 수 있는 편리한 API 를 제공
>브라우저에 맞는 것을 다운로드

1)웹 드라이버 실행
from selenium import webdriver
driver = webdriver.Chrome("C:/Users/user/Download/chromedriver")
2)사이트 접속
driver.get(url)
3)입력양식 채우기
elem = driver.find_element_by_name("q")
elem.clear()
elem.send_keys("pycon")
4)이벤트 처리하기
from selenium.webdriver.common.keys import Keys
elem.send_keys(Keys.RETURN)
5)결과 찾아 출력하기
result_list = driver.find_element_by_css_selector("form h3 > a")
for result in result_list:
    print(result.text)
6)브라우저 종료
driver.close()